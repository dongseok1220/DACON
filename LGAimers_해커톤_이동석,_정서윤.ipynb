{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDypmR1FLAA/Ij9+xIJAII",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgu20191619/DACON/blob/main/LGAimers_%ED%95%B4%EC%BB%A4%ED%86%A4_%EC%9D%B4%EB%8F%99%EC%84%9D%2C_%EC%A0%95%EC%84%9C%EC%9C%A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 설치 \n",
        "!pip install xgboost\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "oOFuE4trCrlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "1Bj8bvhJCt9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "platform.platform()"
      ],
      "metadata": {
        "id": "As02je1AECyO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5290a5e-b44b-4720-ee11-6e5a3af27733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Linux-5.10.147+-x86_64-with-glibc2.31'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/issue.net"
      ],
      "metadata": {
        "id": "ct85P2qREEIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fa2c01-5552-4a80-b5a7-330ec3f6578e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ubuntu 20.04.5 LTS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "print(\"Skleran Version :\", sklearn.__version__)\n",
        "print('Pandas Version :', pd.__version__)\n",
        "print('Numpy Version :', np.__version__)"
      ],
      "metadata": {
        "id": "w7IPLe7HCwZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877d8530-221f-476f-9f85-b4cf5a717dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.16\n",
            "Skleran Version : 1.2.2\n",
            "Pandas Version : 1.4.4\n",
            "Numpy Version : 1.22.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브에서 파일 읽어오기 \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ibTMyP6ECxpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ae962e-0a55-4529-efab-61ddf7be1966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df = pd.read_csv('/content/drive/MyDrive/DACON/스마트 공장 제품 품질 상태 분류 AI 경진대회/train.csv')\n",
        "# test_df = pd.read_csv('/content/drive/MyDrive/DACON/스마트 공장 제품 품질 상태 분류 AI 경진대회/test.csv')\n",
        "# submit = pd.read_csv('/content/drive/MyDrive/DACON/스마트 공장 제품 품질 상태 분류 AI 경진대회/sample_submission.csv')\n",
        "\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/LGAimers/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/LGAimers/test.csv\")\n",
        "submit = pd.read_csv(\"/content/drive/MyDrive/LGAimers/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "emAZu9B_C1Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRODUCT_ID 와 TIMESTAMP 드랍 \n",
        "train_df = train_df.drop(columns=['PRODUCT_ID', 'TIMESTAMP'])\n",
        "test_df = test_df.drop(columns=['PRODUCT_ID', 'TIMESTAMP'])\n",
        "\n",
        "# 코드 공유에 나와있는 라벨 인코딩 \n",
        "qual_col = ['LINE', 'PRODUCT_CODE']\n",
        "\n",
        "for i in qual_col:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(train_df[i])\n",
        "    train_df[i] = le.transform(train_df[i])\n",
        "    \n",
        "    for label in np.unique(test_df[i]): \n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test_df[i] = le.transform(test_df[i]) \n",
        "print('Done.')"
      ],
      "metadata": {
        "id": "uNxbruG3C4oW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c7b1b86-1300-4de4-9510-c6b9d100fc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 값이 하나라면 영향을 주지 않을거라 판단하여 드랍\n",
        "const_cols = train_df.columns[train_df.nunique() == 1] \n",
        "train_df = train_df.drop(const_cols, axis=1)"
      ],
      "metadata": {
        "id": "ZFFPTByvC509"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 \n",
        "# Y_Class, LINE, PRODUCT_CODE 별로 나눈 후 평균 값으로 결측치를 대체하려 했으나, 데이터가 불균형하여 0으로 대체하기로 결정 \n",
        "\n",
        "# Y_Class 별로 나눔 \n",
        "train_class = []\n",
        "for i in range(3) :\n",
        "  train_class.append(train_df.loc[train_df[\"Y_Class\"].isin([i])])\n",
        "\n",
        "# Y_Class 별로 나눈 데이터에서 다시 LINE별로 나눔 \n",
        "line = train_df[\"LINE\"].unique()\n",
        "train_class_line = []\n",
        "for df in train_class :\n",
        "  for l in line :\n",
        "    train_class_line.append(df.loc[df[\"LINE\"].isin([l])])\n",
        "\n",
        "# Y_Class, LINE 별로 나눈 데이터에서 다시 PRODUCT_CODE로 나눔 \n",
        "code = train_df[\"PRODUCT_CODE\"].unique()\n",
        "train_class_line_code = [] \n",
        "for df in train_class_line :\n",
        "  for c in code :\n",
        "    if (len(df.loc[df[\"PRODUCT_CODE\"].isin([c])]) != 0) : \n",
        "      train_class_line_code.append(df.loc[df[\"PRODUCT_CODE\"].isin([c])])\n",
        "\n",
        "# 0으로 결측치 처리하기로 결정 \n",
        "for i in range(len(train_class_line_code)) :\n",
        "  train_class_line_code[i] =  train_class_line_code[i].fillna(0)\n",
        "\n",
        "# 나눴던 데이터를 다시 합침\n",
        "train = pd.concat(train_class_line_code)\n",
        "train = train.reset_index(drop = True)\n",
        "\n",
        "# Y_class와 Y_Quality에 target을 저장 후 드람\n",
        "y_class = train[\"Y_Class\"]\n",
        "y_quality = train[\"Y_Quality\"]\n",
        "train = train.drop(columns=['Y_Class', 'Y_Quality'])\n",
        "\n",
        "# 위에서 전처리를 끝내고 변수 X에 저장, 변수 개수를 cnt 변수에 저장\n",
        "X = train \n",
        "cnt = len(X.columns)"
      ],
      "metadata": {
        "id": "YFH3OseeC_VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlANKpM24Ohd"
      },
      "outputs": [],
      "source": [
        "# 스태킹 앙상블을 진행하고 최종적으로 예측까지 진행하는 함수 \n",
        "def stacking_and_pred(final_X) :\n",
        "  # 최종 학습시킬 train데이터 fianl_X 나눔 \n",
        "  X_train, X_val, y_train, y_val = train_test_split(final_X, y_class, test_size=0.2, random_state=37)\n",
        "\n",
        "  # 가장 보편적으로 많이 사용하는 5가지 모델을 베이스 모델로 설정 \n",
        "  base_models = [\n",
        "      GradientBoostingClassifier(random_state=37),\n",
        "      RandomForestClassifier(n_estimators=2000, random_state=37),\n",
        "      xgb.XGBClassifier(random_state = 37),\n",
        "      LGBMClassifier(random_state = 37),\n",
        "      CatBoostClassifier(random_state=37,verbose=0)\n",
        "  ]\n",
        "  \n",
        "  # 메타 모델로 xgboost 선정 \n",
        "  meta_model = xgb.XGBClassifier(random_state = 37)\n",
        "\n",
        "  # K_Fold를 통한 스태킹 앙상블 진행\n",
        "  n_splits = 5\n",
        "  kf = KFold(n_splits=n_splits, shuffle=True, random_state=37)\n",
        "\n",
        "  X_train_meta = np.zeros((X_train.shape[0], len(base_models)))\n",
        "\n",
        "  # validation으로 메타모델의 f1_score 검증\n",
        "  for i, model in enumerate(base_models):\n",
        "      for train_idx, val_idx in kf.split(X_train):\n",
        "          X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "          y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "          model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "          y_val_pred = model.predict(X_val_fold)\n",
        "\n",
        "          X_train_meta[val_idx, i] = y_val_pred.ravel()\n",
        "\n",
        "  meta_model.fit(X_train_meta, y_train)\n",
        "\n",
        "  X_val_meta = np.zeros((X_val.shape[0], len(base_models)))\n",
        "  for i, model in enumerate(base_models):\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      y_val_pred = model.predict(X_val)\n",
        "      X_val_meta[:, i] = y_val_pred.ravel()\n",
        "\n",
        "  y_pred = meta_model.predict(X_val_meta)\n",
        "\n",
        "  accuracy = f1_score(y_val, y_pred, average='macro')\n",
        "  print(\"F1 score:\", accuracy)\n",
        "\n",
        "  # test데이터 전처리 -> train데이터에서 사용하는 features와 결측치 0으로 대체 \n",
        "  new_test = pd.DataFrame()\n",
        "  cols = final_X.columns \n",
        "\n",
        "  for i in range(len(test_df)) :\n",
        "    row = pd.DataFrame(test_df.iloc[i])\n",
        "    row = np.transpose(row)\n",
        "\n",
        "    test_row = row[cols]\n",
        "    test_row = test_row.fillna(0)\n",
        "    \n",
        "    new_test = pd.concat([new_test, test_row])\n",
        "\n",
        "  # 각 베이스 모델의 test데이터 예측 결과를 담을 변수 \n",
        "  X_test_meta = np.zeros((new_test.shape[0], len(base_models)))\n",
        "\n",
        "  # 전체 데이터로 학습을 시킨 후 예측한 후 X_test_meta에 값 저장 \n",
        "  for i, model in enumerate(base_models):\n",
        "      model.fit(final_X, y_class)\n",
        "      y_test_pred = model.predict(new_test)\n",
        "      X_test_meta[:, i] = y_test_pred.ravel()\n",
        "\n",
        "  # 메타 모델로 최종 y_pred 예측 \n",
        "  y_pred = meta_model.predict(X_test_meta)\n",
        "  # 이후 결과 및 f1_socre 반환 \n",
        "  return y_pred, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 좋은 성능을 보여줄 feature 개수를 찾기 위한 함수 \n",
        "def find(X, bound) : \n",
        "  # Y_Class와 Y_Quality와 나머지 feature들의 pearsons 상관계수를 활용 해 feature_selection 진행 \n",
        "  correlations = []\n",
        "  for feature in X.columns:\n",
        "    correlation, _ = pearsonr(X[feature], y_class)\n",
        "    correlations.append(correlation)\n",
        "\n",
        "  correlations2 = []\n",
        "  for feature in X.columns:\n",
        "      correlation, _ = pearsonr(X[feature], y_quality)\n",
        "      correlations2.append(correlation)\n",
        "\n",
        "  # 5로 나눴을 때 성능이 준수하여 5로 결정 \n",
        "  k = len(X.columns) // 5\n",
        "  top_k_indices = sorted(range(len(correlations)), key=lambda i: abs(correlations[i]), reverse=True)[:k]\n",
        "  top_k_features = X.columns[top_k_indices]\n",
        "\n",
        "  top_k_indices = sorted(range(len(correlations2)), key=lambda i: abs(correlations2[i]), reverse=True)[:k]\n",
        "  top_k_features2 = X.columns[top_k_indices]\n",
        "\n",
        "  top_features = list(OrderedDict.fromkeys(list(top_k_features) + list(top_k_features2)))\n",
        "  new_X = X[top_features]\n",
        "\n",
        "  # 위 new_X를 train으로 여러 모델을 돌려 본 결과 validation이 RandomForest가 가장 높았음 \n",
        "  model = RandomForestClassifier(random_state=37)\n",
        "  model.fit(new_X, y_class)\n",
        "\n",
        "  # 학습 후, feature_importance로 한번 더 feature selection 진행 \n",
        "  importances = model.feature_importances_\n",
        "  feature_importances = pd.DataFrame({'feature': new_X.columns, 'importance': importances})\n",
        "  feature_importances = feature_importances.sort_values('importance', ascending = False)\n",
        "\n",
        "  # feature_importance에서 150~300개를 선택하여 가장 높은 f1_score를 기록한 개수 및 결과값을 저장\n",
        "  for j in bound : \n",
        "    new_cols = feature_importances[:j]['feature']\n",
        "  \n",
        "    print(\"현재 {} 의 feature_importance로 선택된 컬럼 수는 : {}\".format(model, len(new_cols)))\n",
        "\n",
        "    final_X = new_X[new_cols]\n",
        "\n",
        "    y_pred, accuracy = stacking_and_pred(final_X)\n",
        "  \n",
        "    file_name = f\"f1_score={accuracy: .4f},bound={j}).csv\"\n",
        "    submit[\"Y_Class\"] = y_pred\n",
        "    submit.to_csv(\"/content/drive/MyDrive/\" +file_name, index = False)"
      ],
      "metadata": {
        "id": "1gKwIripDGij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mJfKIzCTKHE",
        "outputId": "5e07740e-9fde-4ae4-9858-8b598f245a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bound = [194]\n",
        "\n",
        "find(X, bound)"
      ],
      "metadata": {
        "id": "iRG10Ao6Bm3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2967e3d3-4c35-4fca-9752-35de82fccb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 RandomForestClassifier(random_state=37) 의 feature_importance로 선택된 컬럼 수는 : 194\n",
            "F1 score: 0.7257391675040346\n"
          ]
        }
      ]
    }
  ]
}