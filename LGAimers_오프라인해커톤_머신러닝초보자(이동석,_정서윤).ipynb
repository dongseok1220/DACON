{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1r0pRzvMWxyznVrqTZTekun1xZFJu89cP",
      "authorship_tag": "ABX9TyN3S6RUigpHn6svXI1F4aLn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgu20191619/DACON/blob/main/LGAimers_%EC%98%A4%ED%94%84%EB%9D%BC%EC%9D%B8%ED%95%B4%EC%BB%A4%ED%86%A4_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%B4%88%EB%B3%B4%EC%9E%90(%EC%9D%B4%EB%8F%99%EC%84%9D%2C_%EC%A0%95%EC%84%9C%EC%9C%A4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vEz_nwzezI-",
        "outputId": "65b5b569-cf0c-41b2-cf8a-4f4358935188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 필요한 모듈 설치 \n",
        "!pip install --quiet xgboost\n",
        "!pip install --quiet catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "platform.platform()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jggpzJk5fBxz",
        "outputId": "99a06383-7151-4f5e-d652-fb2951614c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Linux-5.10.147+-x86_64-with-glibc2.31'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/issue.net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgfZlLBEfF64",
        "outputId": "2e30b8ea-12ef-4229-f26f-db3dbb89240a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ubuntu 20.04.5 LTS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "m2dfa3BUe5cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "print(\"Skleran Version :\", sklearn.__version__)\n",
        "print('Pandas Version :', pd.__version__)\n",
        "print('Numpy Version :', np.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu5dP6i2fGfs",
        "outputId": "8d3f8f64-26b4-4947-9d11-c36db788431a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.16\n",
            "Skleran Version : 1.2.2\n",
            "Pandas Version : 1.4.4\n",
            "Numpy Version : 1.22.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이상치 처리\n",
        "def clean_column(col, reference_col=None):\n",
        "    if reference_col is None:\n",
        "        reference_col = col\n",
        "        \n",
        "    z_scores = (reference_col - reference_col.mean()) / reference_col.std()\n",
        "    col_cleaned = col.mask(abs(z_scores) > 3)\n",
        "\n",
        "    most_frequent_value = reference_col.mode()[0]\n",
        "\n",
        "    col_filled = col_cleaned.fillna(most_frequent_value)\n",
        "\n",
        "    return col_filled"
      ],
      "metadata": {
        "id": "TLkrMAKXfLFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스태킹 앙상블을 진행하고 최종적으로 예측까지 진행하는 함수 \n",
        "def stacking_and_pred(final_X, new_test) :\n",
        "  # 최종 학습시킬 train데이터 fianl_X 나눔 \n",
        "  X_train, X_val, y_train, y_val = train_test_split(final_X, y_class, test_size=0.2, random_state=37)\n",
        "\n",
        "  # 가장 보편적으로 많이 사용하는 5가지 모델을 베이스 모델로 설정 \n",
        "  base_models = [\n",
        "      GradientBoostingClassifier(random_state=37),\n",
        "      RandomForestClassifier(n_estimators=2000, random_state=37),\n",
        "      xgb.XGBClassifier(random_state = 37),\n",
        "      LGBMClassifier(random_state = 37),\n",
        "      CatBoostClassifier(random_state=37, verbose = 0), \n",
        "  ]\n",
        "  \n",
        "  # 메타 모델로 xgboost 선정 \n",
        "  meta_model = xgb.XGBClassifier(random_state = 37)\n",
        "\n",
        "  # K_Fold를 통한 스태킹 앙상블 진행\n",
        "  n_splits = 5\n",
        "  kf = KFold(n_splits=n_splits, shuffle=True, random_state=37)\n",
        "\n",
        "  X_train_meta = np.zeros((X_train.shape[0], len(base_models)))\n",
        "\n",
        "  # validation으로 메타모델의 f1_score 검증\n",
        "  for i, model in enumerate(base_models):\n",
        "      for train_idx, val_idx in kf.split(X_train):\n",
        "          X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "          y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "          model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "          y_val_pred = model.predict(X_val_fold)\n",
        "\n",
        "          X_train_meta[val_idx, i] = y_val_pred.ravel()\n",
        "\n",
        "  meta_model.fit(X_train_meta, y_train)\n",
        "\n",
        "  X_val_meta = np.zeros((X_val.shape[0], len(base_models)))\n",
        "  for i, model in enumerate(base_models):\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      y_val_pred = model.predict(X_val)\n",
        "      X_val_meta[:, i] = y_val_pred.ravel()\n",
        "\n",
        "  y_pred = meta_model.predict(X_val_meta)\n",
        "\n",
        "  accuracy = f1_score(y_val, y_pred, average='macro')\n",
        "  print(\"F1 score:\", accuracy)\n",
        "\n",
        "  # 각 베이스 모델의 test데이터 예측 결과를 담을 변수 \n",
        "  X_test_meta = np.zeros((new_test.shape[0], len(base_models)))\n",
        "\n",
        "  # 전체 데이터로 학습을 시킨 후 예측한 후 X_test_meta에 값 저장 \n",
        "  for i, model in enumerate(base_models):\n",
        "      model.fit(final_X, y_class)\n",
        "      y_test_pred = model.predict(new_test)\n",
        "      X_test_meta[:, i] = y_test_pred.ravel()\n",
        "\n",
        "  # 메타 모델로 최종 y_pred 예측 \n",
        "  y_pred = meta_model.predict(X_test_meta)\n",
        "  # 이후 결과 및 f1_socre 반환 \n",
        "  return y_pred, accuracy"
      ],
      "metadata": {
        "id": "SQi4PesFfTCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 좋은 성능을 보여줄 feature 개수를 찾기 위한 함수 \n",
        "def find(X, bound, new_test) : \n",
        "  # Y_Class와 Y_Quality와 나머지 feature들의 pearsons 상관계수를 활용 해 feature_selection 진행 \n",
        "  correlations = []\n",
        "  for feature in X.columns:\n",
        "    correlation, _ = pearsonr(X[feature], y_class)\n",
        "    correlations.append(correlation)\n",
        "\n",
        "  correlations2 = []\n",
        "  for feature in X.columns:\n",
        "      correlation, _ = pearsonr(X[feature], y_quality)\n",
        "      correlations2.append(correlation)\n",
        "\n",
        "  k = len(X.columns) // 3\n",
        "  top_k_indices = sorted(range(len(correlations)), key=lambda i: abs(correlations[i]), reverse=True)[:k]\n",
        "  top_k_features = X.columns[top_k_indices]\n",
        "\n",
        "  top_k_indices = sorted(range(len(correlations2)), key=lambda i: abs(correlations2[i]), reverse=True)[:k]\n",
        "  top_k_features2 = X.columns[top_k_indices]\n",
        "\n",
        "  top_features = list(OrderedDict.fromkeys(list(top_k_features) + list(top_k_features2)))\n",
        "  new_X = X[top_features]\n",
        "\n",
        "  # 위 new_X를 train으로 여러 모델을 돌려 본 결과 validation이 RandomForest가 가장 높았음 \n",
        "  model = RandomForestClassifier(random_state=37)\n",
        "  model.fit(new_X, y_class)\n",
        "\n",
        "  # 학습 후, feature_importance로 한번 더 feature selection 진행 \n",
        "  importances = model.feature_importances_\n",
        "  feature_importances = pd.DataFrame({'feature': new_X.columns, 'importance': importances})\n",
        "  feature_importances = feature_importances.sort_values('importance', ascending = False)\n",
        "\n",
        "  # feature_importance에서 150~300개를 선택하여 가장 높은 f1_score를 기록한 개수 및 결과값을 저장\n",
        "  for j in bound : \n",
        "    new_cols = feature_importances[:j]['feature']\n",
        "  \n",
        "    print(\"현재 {} 의 feature_importance로 선택된 컬럼 수는 : {}\".format(model, len(new_cols)))\n",
        "\n",
        "    final_X = new_X[new_cols]\n",
        "    new_test = new_test[new_cols]\n",
        "    y_pred, accuracy = stacking_and_pred(final_X, new_test)\n",
        "  \n",
        "    file_name = f\"prepro-t(rb)_cat_f1_score={accuracy: .4f},bound={j}.csv\"\n",
        "    submit[\"Y_Class\"] = y_pred\n",
        "    submit.to_csv(\"/content/drive/MyDrive/\" +file_name, index = False)"
      ],
      "metadata": {
        "id": "yZN-elZyfUqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/test.csv\")\n",
        "submit = pd.read_csv(\"/content/drive/MyDrive/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "3NpqrCPze75w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX2TwB1njYIa",
        "outputId": "35673881-ac22-48eb-912b-5b6170372c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1132, 3331)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()"
      ],
      "metadata": {
        "id": "2kYpvXY2jlCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRODUCT_ID 와 TIMESTAMP 드랍 \n",
        "train_df = train_df.drop(columns=['PRODUCT_ID'])\n",
        "test_df = test_df.drop(columns=['PRODUCT_ID'])\n",
        "\n",
        "# 코드 공유에 나와있는 라벨 인코딩 \n",
        "qual_col = ['LINE', 'PRODUCT_CODE']\n",
        "\n",
        "for i in qual_col:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(train_df[i])\n",
        "    train_df[i] = le.transform(train_df[i])\n",
        "    \n",
        "    for label in np.unique(test_df[i]): \n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test_df[i] = le.transform(test_df[i]) \n",
        "print('Done.')"
      ],
      "metadata": {
        "id": "xCedeZkVfZQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모두 nan값인 column\n",
        "all_nan_cols = train_df.columns[train_df.isna().all()].tolist()\n",
        "\n",
        "print('Columns with all missing values:')\n",
        "print(all_nan_cols)\n",
        "print(len(all_nan_cols))\n",
        "\n",
        "# 유일한 값이 하나만 존재하는 column\n",
        "const_cols = train_df.columns[train_df.nunique() == 1].tolist()\n",
        "\n",
        "print('Columns with only one unique value:')\n",
        "print(const_cols)\n",
        "print(len(const_cols))\n",
        "\n",
        "train_df = train_df.drop(all_nan_cols, axis=1)\n",
        "train_df = train_df.drop(const_cols, axis=1)\n",
        "\n",
        "print(len(train_df.columns))\n",
        "# *2drop -> 2794(나와야함)"
      ],
      "metadata": {
        "id": "B6QHphV0fbTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train과 동일한 전처리 \n",
        "test_df = test_df.drop(all_nan_cols, axis=1)\n",
        "test_df = test_df.drop(const_cols, axis=1)"
      ],
      "metadata": {
        "id": "KvlxmODwfnAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 \n",
        "# Y_Class, LINE, PRODUCT_CODE 별로 나눈 후 평균 값으로 결측치를 대체하려 했으나, 데이터가 불균형하여 0으로 대체하기로 결정 \n",
        "\n",
        "# Y_Class 별로 나눔 \n",
        "train_class = []\n",
        "for i in range(3) :\n",
        "  train_class.append(train_df.loc[train_df[\"Y_Class\"].isin([i])])\n",
        "\n",
        "# Y_Class 별로 나눈 데이터에서 다시 LINE별로 나눔 \n",
        "line = train_df[\"LINE\"].unique()\n",
        "train_class_line = []\n",
        "for df in train_class :\n",
        "  for l in line :\n",
        "    train_class_line.append(df.loc[df[\"LINE\"].isin([l])])\n",
        "\n",
        "# Y_Class, LINE 별로 나눈 데이터에서 다시 PRODUCT_CODE로 나눔 \n",
        "code = train_df[\"PRODUCT_CODE\"].unique()\n",
        "train_class_line_code = [] \n",
        "for df in train_class_line :\n",
        "  for c in code :\n",
        "    if (len(df.loc[df[\"PRODUCT_CODE\"].isin([c])]) != 0) : \n",
        "      train_class_line_code.append(df.loc[df[\"PRODUCT_CODE\"].isin([c])])\n",
        "\n",
        "# 0으로 결측치 처리하기로 결정 \n",
        "for i in range(len(train_class_line_code)) :\n",
        "  train_class_line_code[i] =  train_class_line_code[i].fillna(0)\n",
        "\n",
        "# 나눴던 데이터를 다시 합침\n",
        "train = pd.concat(train_class_line_code)\n",
        "train = train.reset_index(drop = True)\n",
        "\n",
        "# Y_class와 Y_Quality에 target을 저장 후 드람\n",
        "y_class = train[\"Y_Class\"]\n",
        "y_quality = train[\"Y_Quality\"]\n",
        "train = train.drop(columns=['Y_Class', 'Y_Quality'])\n",
        "\n",
        "X = train \n",
        "\n",
        "test_df = test_df.fillna(0)"
      ],
      "metadata": {
        "id": "_ykNRz8PfpaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이상치 처리 \n",
        "X_t = X.apply(clean_column)\n",
        "test_df = test_df.apply(lambda col: clean_column(col, reference_col=X[col.name]))"
      ],
      "metadata": {
        "id": "XqiuVDfwfvfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이상치에 강한 로버스트 스케일링 \n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "rb = RobustScaler()\n",
        "rb.fit(X_t)\n",
        "train_scaled = rb.transform(X_t)\n",
        "test_scaled = rb.transform(test_df)\n",
        "\n",
        "X_train = pd.DataFrame(train_scaled)\n",
        "X_test = pd.DataFrame(test_scaled)"
      ],
      "metadata": {
        "id": "rWYWPNc1fxJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bound = range(208,209)\n",
        "\n",
        "find(X_train, bound, X_test)"
      ],
      "metadata": {
        "id": "GrpbMHptf0u5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}